{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = np.fromfile(\n",
    "\"/data/paloma/tokenized_paloma_c4_100_domains_validation.bin\",\n",
    "dtype=np.uint16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/c-cye/uv-envs/assignment4-data/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = np.fromfile(\"/data/c-cye/assignment4-data/cc_tokenized/CC-MAIN-20250418151910-20250418181910-00091.bin\",dtype=np.uint16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "testtext = tokenizer.decode(tokens)\n",
    "docs = testtext.split(\"<|endoftext|>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testfile = \"/data/c-cye/assignment4-data/cc_filtered/CC-MAIN-20250418151910-20250418181910-00091.txt\"\n",
    "with open(testfile, \"r\") as f:\n",
    "    testtext = f.read()\n",
    "tokenized = tokenizer.tokenize(testtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_paloma = tokenizer.decode(data)\n",
    "# save to file\n",
    "with open(\"decoded_paloma.txt\", \"w\") as f:\n",
    "    f.write(decoded_paloma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14059"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean newlines, whitespace, and empty strings from decoded_paloma\n",
    "docs = [doc.strip().replace(\"\\n\", \" \") for doc in decoded_paloma.split(\"<|endoftext|>\") if doc.strip()]\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Paloma Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write fasttext file\n",
    "with open(\"fasttext_paloma.txt\", \"w\") as f:\n",
    "    for doc in docs:\n",
    "        f.write(f\"__label__paloma {doc}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import Counter\n",
    "from typing import List, Tuple\n",
    "\n",
    "def read_fasttext_file(filepath: str) -> List[str]:\n",
    "    \"\"\"Read FastText formatted file and return list of lines.\"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        return [line.strip() for line in f if line.strip()]\n",
    "\n",
    "def extract_label(line: str) -> str:\n",
    "    \"\"\"Extract label from FastText formatted line (assumes __label__xxx format).\"\"\"\n",
    "    return line.split()[0]\n",
    "\n",
    "def balance_data(data: List[str]) -> List[str]:\n",
    "    \"\"\"Balance the dataset by undersampling to match the minority class.\"\"\"\n",
    "    # group lines by label\n",
    "    label_groups = {}\n",
    "    for line in data:\n",
    "        label = extract_label(line)\n",
    "        if label not in label_groups:\n",
    "            label_groups[label] = []\n",
    "        label_groups[label].append(line)\n",
    "    \n",
    "    # find minimum class size\n",
    "    min_size = min(len(group) for group in label_groups.values())\n",
    "    \n",
    "    print(f\"Original distribution: {[(label, len(group)) for label, group in label_groups.items()]}\")\n",
    "    print(f\"Balancing to {min_size} samples per class\")\n",
    "    \n",
    "    # sample min_size examples from each class\n",
    "    balanced_data = []\n",
    "    for label, group in label_groups.items():\n",
    "        balanced_data.extend(random.sample(group, min_size))\n",
    "    \n",
    "    return balanced_data\n",
    "\n",
    "def shuffle_and_split_fasttext(file1_path: str, file2_path: str, \n",
    "                              train_split: float = 0.8, \n",
    "                              balance: bool = True,\n",
    "                              output_train: str = 'train.txt',\n",
    "                              output_test: str = 'test.txt',\n",
    "                              random_seed: int = 42) -> None:\n",
    "    \"\"\"\n",
    "    process two fasttext files: combine, shuffle, balance, and split.\n",
    "    \n",
    "    Args:\n",
    "        file1_path: Path to first FastText file\n",
    "        file2_path: Path to second FastText file  \n",
    "        train_split: Fraction for training set (0.0 to 1.0)\n",
    "        balance: Whether to balance classes by undersampling\n",
    "        output_train: Output path for training set\n",
    "        output_test: Output path for test set\n",
    "        random_seed: Random seed for reproducibility\n",
    "    \"\"\"\n",
    "    \n",
    "    # set random seed for reproducibility\n",
    "    random.seed(random_seed)\n",
    "    \n",
    "    # read both files\n",
    "    print(\"reading files...\")\n",
    "    data1 = read_fasttext_file(file1_path)\n",
    "    data2 = read_fasttext_file(file2_path)\n",
    "    \n",
    "    # combine data\n",
    "    all_data = data1 + data2\n",
    "    print(f\"combined {len(data1)} + {len(data2)} = {len(all_data)} samples\")\n",
    "    \n",
    "    # show original distribution\n",
    "    labels = [extract_label(line) for line in all_data]\n",
    "    print(f\"label distribution: {Counter(labels)}\")\n",
    "\n",
    "    # rebalance classes\n",
    "    if balance:\n",
    "        all_data = balance_data(all_data)\n",
    "        labels = [extract_label(line) for line in all_data]\n",
    "        print(f\"Balanced distribution: {Counter(labels)}\")\n",
    "    \n",
    "    # shuffle the combined data\n",
    "    random.shuffle(all_data)\n",
    "    print(\"data shuffled\")\n",
    "    \n",
    "    # split into train/test\n",
    "    split_idx = int(len(all_data) * train_split)\n",
    "    train_data = all_data[:split_idx]\n",
    "    test_data = all_data[split_idx:]\n",
    "    \n",
    "    print(f\"split: {len(train_data)} train, {len(test_data)} test\")\n",
    "    \n",
    "    # check distribution in splits\n",
    "    train_labels = [extract_label(line) for line in train_data]\n",
    "    test_labels = [extract_label(line) for line in test_data]\n",
    "    print(f\"train distribution: {Counter(train_labels)}\")\n",
    "    print(f\"test distribution: {Counter(test_labels)}\")\n",
    "    \n",
    "    # write output files\n",
    "    with open(output_train, 'w', encoding='utf-8') as f:\n",
    "        for line in train_data:\n",
    "            f.write(line + '\\n')\n",
    "    \n",
    "    with open(output_test, 'w', encoding='utf-8') as f:\n",
    "        for line in test_data:\n",
    "            f.write(line + '\\n')\n",
    "    \n",
    "    print(f\"Files saved: {output_train}, {output_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading files...\n",
      "combined 15000 + 14059 = 29059 samples\n",
      "label distribution: Counter({'__label__high-quality': 15000, '__label__paloma': 14059})\n",
      "Original distribution: [('__label__high-quality', 15000), ('__label__paloma', 14059)]\n",
      "Balancing to 14059 samples per class\n",
      "Balanced distribution: Counter({'__label__high-quality': 14059, '__label__paloma': 14059})\n",
      "data shuffled\n",
      "split: 25306 train, 2812 test\n",
      "train distribution: Counter({'__label__paloma': 12674, '__label__high-quality': 12632})\n",
      "test distribution: Counter({'__label__high-quality': 1427, '__label__paloma': 1385})\n",
      "Files saved: paloma.train, paloma.test\n"
     ]
    }
   ],
   "source": [
    "shuffle_and_split_fasttext(\n",
    "        file1_path='positive_data_cleaner.txt',\n",
    "        file2_path='fasttext_paloma.txt',\n",
    "        train_split=0.9,\n",
    "        balance=True,\n",
    "        output_train='paloma.train',\n",
    "        output_test='paloma.test',\n",
    "        random_seed=42\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100.0% Trials:    9 Best score:  0.982219 ETA:   0h 0m 0s\n",
      "Training again with best arguments\n",
      "Read 20M words\n",
      "Number of words:  897603\n",
      "Number of labels: 2\n",
      "Progress: 100.0% words/sec/thread:  519900 lr:  0.000000 avg.loss:  0.022311 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "model = fasttext.train_supervised(input='paloma.train', autotuneValidationFile='paloma.test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model('paloma.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_test = read_fasttext_file(\"positive_data_filtered.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3450561/2847972384.py:2: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  vals = [float(val) for val in vals]\n"
     ]
    }
   ],
   "source": [
    "labels, vals = model.predict(positive_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_DIR = '/data/c-cye/assignment4-data/cc_filtered'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect all jsons and average fields\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "\n",
    "jsons = glob.glob(os.path.join(OUT_DIR, \"*.json\"))\n",
    "\n",
    "data = [json.load(open(f)) for f in jsons]\n",
    "fields = data[0].keys()\n",
    "average = {key: 0 for key in fields}\n",
    "for field in fields:\n",
    "    for d in data:\n",
    "        average[field] += d[field]\n",
    "\n",
    "total_records = average['total_records']\n",
    "# for field in fields:\n",
    "#     average[field] /= total_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_records': 113329836,\n",
       " 'after_language_filter': 45948917,\n",
       " 'after_gopher_filter': 31268737,\n",
       " 'after_nsfw_filter': 31121354,\n",
       " 'after_toxic_filter': 31041103,\n",
       " 'after_quality_filter': 3903567,\n",
       " 'after_dedup': 3882421}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assignment4-data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
